{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Python the Hard Way - Session 2\n",
    "Toronto Data Literacy Group\n",
    "\n",
    "Creator: Cindy Zhong\n",
    "\n",
    "Date: January 09, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading The Data\n",
    "\n",
    "The data for the file can be downloaded from the github repository. \n",
    "If you want to get the data from Twitter youself, it is created using the code from Session 1. https://github.com/cindyzhong/trt_data_lit_grp_python/tree/master/Lesson1\n",
    "\n",
    "Next, read the tab-delimited file into Python. To do this, we can use the pandas package which provides the read_csv function for easily reading and writing data files. If you haven't used pandas before, you may need to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "tweet_df = pd.read_csv(\"tweet_sample.csv\", delimiter=\",\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A look at the dimension of the dataframe\n",
    "tweet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A look at the columns of the dataframe\n",
    "tweet_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A look at sample data\n",
    "tweet_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also look at a ramdom sample of the rows\n",
    "tweet_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's use one tweet as an example\n",
    "tweet_eg = tweet_df['tweet_body'][8]\n",
    "tweet_eg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and Pre-Processing The Texts\n",
    "We are interested in the text of the tweets.\n",
    "The unique thing about text analytics is there is no standard way of pre-processing the data. Depending on the problem you are trying to solve, the pre-processing can be different.\n",
    "In most cases, it consist of the following components:\n",
    "- Removing Unwanted Characters\n",
    "- Removing Punctuations\n",
    "- Removing Numbers\n",
    "- Standardizing Cases\n",
    "- Removing Stopwords\n",
    "We will explain each of them in our session.\n",
    "We will be using a package called NLTK (Natural Language Toolkit), and a package called re (Regular Expression) extensively in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Text Cleaning Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regular Expression itself is a very useful skill to learn.\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A lot of the tweets contains reference urls, we want to remove them first\n",
    "def remove_url(text):\n",
    "\ttext = re.sub('http://[^ ]*', '', text)\n",
    "\ttext = re.sub('https://[^ ]*', '', text)\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the function on our sample tweet\n",
    "tweet_eg = remove_url(tweet_eg)\n",
    "tweet_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing the at users\n",
    "def remove_at_user(text):\n",
    "\timport re\n",
    "\treturn re.sub('@[^\\s]+','', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_eg = remove_at_user(tweet_eg)\n",
    "tweet_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now try to write a function to remove the retweet 'RT'\n",
    "def remove_rt(text):\n",
    "    text = re.sub('RT', '', text, count=1)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_eg = remove_rt(tweet_eg)\n",
    "tweet_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's remove the punctuations and numbers, basically all the non letters for now\n",
    "def remove_non_letters(text):\n",
    "\treturn re.sub(\"[^a-zA-Z]\", \" \", text) \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_eg = remove_non_letters(tweet_eg)\n",
    "tweet_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We might want to remove some extra blanks\n",
    "def remove_extra_blanks(text):\n",
    "\ttext = re.sub('\\n', ' ', text)\n",
    "\ttext = re.sub(\" +\",\" \",text).strip() #remove extra spaces\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_eg = remove_extra_blanks(tweet_eg)\n",
    "tweet_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardizing Cases\n",
    "def all_lower_case(text):\n",
    "\treturn text.lower()\n",
    "\n",
    "tweet_eg = all_lower_case(tweet_eg)\n",
    "tweet_eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, let's put all of the above cleaning functions together\n",
    "def my_text_cleanser(text):\n",
    "    if isinstance(text,basestring):\n",
    "        text = text.encode('utf-8')\n",
    "        text = remove_url(text)\n",
    "        text = remove_rt(text)\n",
    "        text = remove_non_letters(text)\n",
    "        text = remove_extra_blanks(text)\n",
    "        text = all_lower_case(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will apply the text cleanser to our 'tweet_body' column, using a very commonly used function in pandas 'apply'\n",
    "tweet_df['tweet_body_clean'] = tweet_df.tweet_body.apply(my_text_cleanser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at the old column and the cleaned new column\n",
    "tweet_df[['tweet_body','tweet_body_clean']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords\n",
    "Stopwords are words that occur in a sentence often that do not carry any meanings, for example, 'am','and','the'.\n",
    "We often want to remove these words when we are doing text analytics.\n",
    "To do this, we will use NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you haven't done so already, download the nltk's corpus for stopwords\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the stop word list\n",
    "from nltk.corpus import stopwords \n",
    "print (stopwords.words(\"english\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    meaningful_words = [w for w in words if not w in stopwords.words(\"english\") ]\n",
    "    return meaningful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_eg = remove_stopwords(tweet_eg)\n",
    "tweet_eg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Stemming\n",
    "In linguistic morphology and information retrieval, stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root formâ€”generally a written word form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Examples of stemmed words\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "print (snowball_stemmer.stem('interaction'))\n",
    "print (snowball_stemmer.stem('interact'))\n",
    "print (snowball_stemmer.stem('interactions'))\n",
    "print (snowball_stemmer.stem('interactivity'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Lemmatization\n",
    "Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print (wordnet_lemmatizer.lemmatize('interaction'))\n",
    "print (wordnet_lemmatizer.lemmatize('interact'))\n",
    "print (wordnet_lemmatizer.lemmatize('interactions'))\n",
    "print (wordnet_lemmatizer.lemmatize('interactivity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will be using the lemmatizer for our purpose\n",
    "def lemmatizer(words):\n",
    "    return [wordnet_lemmatizer.lemmatize(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_text_tokenizer(text):\n",
    "    words = remove_stopwords(text)\n",
    "    words = lemmatizer(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's apply the functions above to our cleaned tweet\n",
    "tweet_df['tweet_body_terms'] = tweet_df.tweet_body_clean.apply(my_text_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at what we've done so far\n",
    "tweet_df[['tweet_body','tweet_body_clean','tweet_body_terms']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Text Analytics on Tweets\n",
    "With the text pre-processed, we can now do some simple but interesting analytics on the tweets, in this session, we will look at for Trump and Hilary \n",
    "- Term Collocation\n",
    "- Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since we will be creating statistics at user level, we group the dataframe by users\n",
    "users_df = tweet_df.groupby('handle').agg({'tweet_body_terms':sum,'tweet_body_clean':lambda x: ' '.join(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Collocations\n",
    "Collocations are partly or fully fixed expressions that become established through repeated context-dependent use. \n",
    "For example, 'crystal clear', 'middle management', and 'plastic surgery' are examples of collocated pairs of words.\n",
    "We are interested in looking at term collocations the context gives us a better insight about the meaning of a term, supporting applications such as word disambiguation or semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find top collocation in the tweets\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "def top_collocation_text(words):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(words)\n",
    "    finder.apply_freq_filter(5)\n",
    "    return finder.nbest(bigram_measures.pmi, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see what are the most often talked about terms for Hilary and Trump\n",
    "users_df['top_collocation_text'] = users_df.tweet_body_terms.apply(top_collocation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (users_df['top_collocation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Diversity\n",
    "Lexical diversity is a measure of how many different words that are used in a text.\n",
    "The more varied a vocabulary a text possesses, the higher lexical diversity.\n",
    "For a text to be highly lexically diverse, the speaker or writer has to use many\n",
    "different words, with littie repetition of the words already used. \n",
    "The lexical diversity of a given text is defined as the ratio of total number of words to the number of different unique word stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(words):\n",
    "    return 1.0*len(set(words))/(len(words)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_df['lexical_diversity'] = users_df.tweet_body_terms.apply(lexical_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (users_df['lexical_diversity'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
